









[음성 데이터를 활용한 직관적 미디 스케치 시스템]

[중간보고서]


2025. 4. 21



201920185 김지환
목차
요약	2
1. 개요	3
1-1. 문제 정의	3
1-2 문제 해결 방법	3
2. 진행방법	5
3. 진행 현황	6
3-1. Step 1: 프로젝트 목표 태스크 3가지 정의	7
3-2. Step 2: 평가 셋 제작	8
3-3. Step 3: 허밍 to MIDI 변환을 위한 모델 후보군 탐색	10
3-3-1. 모델 후보군 탐색	10
3-3-1-1. 전통적인 신호처리 기법	10
3-3-1-2. 딥러닝 기반 신호처리 기법	12
3-3-1-3. 모델 후보군 탐색 요약	16
3-3-2. 모델 후보군 테스트 결과	17
3-4. Step 4: MIDI 후처리 보정을 위한 방법 탐색	18
3-5. Step 7: VST 플러그인 디자인 프로토타입 제작	20
4. 진행 예정 내용	21
Reference	22

<표 목차>
표 1. 프로젝트 진행 계획 .......................................................................................................................................... 6
표 2. 프로젝트 진행 완료 여부 ............................................................................................................................... 7
표 3. 평가셋 구성 상세 계획 ................................................................................................................................. 10
표 4. 각 모델 예상 요인 평가 ............................................................................................................................... 18
<그림 목차>
그림 1. FFT 기반 허밍 to MIDI 변환 .................................................................................................................. 18
그림 2. YIN 기반 허밍 to MIDI 변환 ................................................................................................................. 18
그림 3. HPS 기반 허밍 to MIDI 변환 ................................................................................................................ 18
그림 4. 필터링 전후 MIDI 노트 비교 예시 ...................................................................................................... 19
그림 5. 사용자 인터페이스 프로토타입 화면 ................................................................................................. 21









    



요약
본 프로젝트는 사용자의 허밍(음성)을 미디(MIDI)로 변환하여 누구나 직관적으로 작곡할 수 있도록 돕는 시스템을 개발하는 것을 목표로 한다. 이는 음악적 지식이 부족한 사용자도 손쉽게 창작에 접근할 수 있도록 돕기 위한 시도로, 기존 작곡 프로그램의 진입 장벽을 해소하는 데 중점을 둔다.
이를 위해 먼저 허밍 데이터를 기반으로 음정과 리듬 정보를 정확하게 추출할 수 있는 다양한 신호처리 및 딥러닝 기법을 비교·분석하였다. FFT, YIN 등의 경량 신호처리 방식이 상대적으로 높은 성능을 보이며 실제 구현에 적합하다는 결론을 얻었고, 이에 기반한 변환 모델을 중심으로 시스템을 설계하였다. 또한, 변환된 미디의 품질을 개선하기 위해 MIDI 후처리 알고리즘을 고안하고, 향후에는 딥러닝 기반 정제 모델을 추가할 계획이다.
사용자 접근성을 높이기 위해 본 시스템은 DAW 환경에서 사용할 수 있는 VST 플러그인 형태로 구현된다. 직관적인 UI를 갖춘 프로토타입을 제작하였고, 녹음된 허밍의 파형과 미디 노트를 시각적으로 확인하며 다양한 후처리 파라미터를 조절할 수 있도록 구성하였다.
현재까지 전체 8단계 중 절반 이상의 과정을 완료하였으며, 향후에는 모델 성능 평가와 실제 플러그인 구현을 통해 프로젝트를 마무리할 계획이다.

1. 개요
1-1. 문제 정의
본 프로젝트의 문제 정의는 허밍을 미디(MIDI) 데이터로 변환하는 과정에서 발생하는 정확성 문제를 해결하는 것이다. 기존의 작곡 프로그램은 사용자가 악기나 키보드 등을 활용해 작곡을 해야 하므로, 음악적 지식이 부족한 사람들에게는 접근이 어려운 진입 장벽이 존재한다. 이에 따라 본 프로젝트는 보컬 데이터를 미디로 변환하여 사용자가 직관적으로 작곡할 수 있도록 지원하는 시스템을 개발하는 데 초점을 맞춘다.
하지만 허밍을 미디로 변환하는 과정에서는 여러 기술적인 도전 과제가 존재한다. 예를 들어, 허밍의 정확한 음정을 추출하는 것, 다양한 리듬 패턴을 인식하는 것, 그리고 소음이나 왜곡이 포함된 데이터를 처리하는 등의 문제를 해결해야 한다. 또한, 변환된 미디 데이터를 DAW(Digital Audio Workstation) 상에서 사용할 수 있도록 VST 플러그인 형식으로 제공하려면, 실시간 변환과 높은 성능을 보장하는 시스템을 개발해야 한다. 이러한 문제를 해결하기 위해 본 프로젝트는 허밍에서 추출된 음성 데이터를 미디 형식으로 변환하는 모델의 정확도를 평가하고, 이를 최적화하여 실제로 사용 가능한 시스템을 구현하는 것을 목표로 한다.
1-2 문제 해결 방법 
본 프로젝트의 문제 해결 방법은 허밍을 미디 데이터로 변환하는 시스템을 DAW에서 실제로 사용할 수 있는 VST 플러그인 형태로 구현하는 데 중점을 둔다. 이를 통해 사용자는 보컬 입력을 미디로 변환하고, 이를 기반으로 다양한 악기 소리를 자유롭게 표현할 수 있게 된다.
첫째, 허밍 데이터의 정확한 추출을 위해 음성 신호 처리 기법을 활용한다. 주파수 분석(FFT 등)과 피치 추정 알고리즘을 사용하여 허밍의 음정과 리듬을 추출하고, 이를 미디 형식으로 변환하는 알고리즘을 개발한다. 또한, 사용자의 허밍에서 발생할 수 있는 잡음이나 왜곡을 최소화하기 위해 노이즈 필터링과 피치 보정 알고리즘을 적용한다. 변환은 실시간으로 이루어져, 사용자가 즉시 변환된 미디 데이터를 활용할 수 있도록 하며, 반복적인 학습을 통해 정확도를 최적화한다.
둘째, 변환 시스템을 DAW에서 실제로 사용할 수 있도록 VST(가상 악기) 플러그인 형식으로 구현한다. VST 플러그인은 사용자가 허밍을 입력하면 이를 미디로 변환하고, 변환된 미디 데이터를 다양한 악기 소리로 표현할 수 있도록 한다. 이를 위해 플러그인의 구조를 설계하고, 직관적인 사용자 인터페이스(UI)를 구현하여 사용자가 손쉽게 작곡할 수 있도록 한다. 또한, 미디 출력 기능을 내장하여 DAW 내에서 실시간으로 작곡할 수 있는 환경을 제공한다.
이를 통해 본 프로젝트는 음악적 지식이 부족한 사람들도 손쉽게 작곡을 시작할 수 있도록 돕는 직관적인 시스템을 구현하며, DAW 환경에서 실제로 활용 가능한 VST 플러그인 형태로 제공된다.

2. 진행방법
본 프로젝트는 사용자의 허밍을 미디로 변환하고 이를 DAW 환경에서 실제로 활용할 수 있도록 하는 시스템을 개발하는 것을 목표로 하며, 이를 위해 다음 [표1]과 같은 단계로 진행된다. 먼저, 전체 프로젝트의 방향성과 요구 기능을 기반으로 세 가지 핵심 태스크를 수립하고, 각각의 목표에 따라 평가 기준을 정의한다. 이어서, 시스템 성능을 정량적으로 검증하기 위해 다양한 유형의 허밍 데이터를 포함한 평가셋을 제작한다.
그 후, 허밍을 미디 데이터로 변환하기 위한 모델 후보군을 탐색하고, 실험을 통해 최종 모델을 결정한다. 변환된 미디 데이터의 품질 향상을 위해 미디 후처리 및 보정 기법도 함께 탐색하며, 최종적으로 선택된 모델은 반복적인 피드백과 개선 과정을 거쳐 완성된다. 완성된 모델은 앞서 구축한 평가셋에 적용되어 세부 태스크별 정확도 및 성능 평가를 진행한다.
이후, 실제 작곡 환경에서의 사용을 고려해 VST 플러그인 디자인의 프로토타입을 제작하고, 직관적인 UI 및 주요 기능 구성을 설계한다. 마지막으로, 이를 바탕으로 플러그인 개발을 진행하고, DAW 상에서의 작동을 확인하는 시연을 통해 프로젝트를 마무리한다.
단계
내용
Step 1
프로젝트 목표 태스크 3가지 정의
Step 2
평가 셋 제작
Step 3
허밍 to MIDI 변환을 위한 모델 후보군 탐색 및 최종 결정
Step 4
MIDI 후처리 보정을 위한 방법 탐색
Step 5
모델 완성 및 피드백
Step 6
완성된 모델 평가셋에 적용 후 태스크 평가
Step 7
VST 플러그인 디자인 프로토타입 제작
Step 8
플러그인 개발 및 시연

[표1] 프로젝트 진행 계획

3. 진행 현황
현재 본 프로젝트는 [표2]와 같이 전체 8단계 중 절반 이상의 주요 단계를 완료한 상태이다. 먼저 Step 1에서 프로젝트의 목표를 구체화하고, 이를 세 가지 핵심 태스크로 정의하였다. 이어 Step 2에서는 평가 기준에 맞춰 총 21개의 허밍-미디 페어로 구성된 평가셋을 제작하였으며, 이는 정상 허밍, 노이즈가 포함된 허밍, 음정 보정이 필요한 허밍 등 세 가지 유형으로 나뉘어 있다. Step 3에서는 허밍을 미디로 변환할 수 있는 다양한 모델을 조사하고 실험한 결과, 성능과 안정성이 우수한 모델을 최종 후보로 선정하였다. 이어서 Step 4에서는 변환된 미디 데이터의 정확성과 자연스러움을 높이기 위한 후처리 및 보정 기법을 적용하여, 모델의 출력을 더욱 실용적인 형태로 개선하였다. 또한, Step 7에 해당하는 VST 플러그인 디자인 프로토타입 역시 완료되어, 직관적인 UI 설계와 기능 구조 정의가 마무리된 상태이다. 현재는 Step 5의 모델 완성 및 피드백 단계를 진행 중이며, 이후 평가셋을 통해 성능을 검증한 뒤, 실제 작곡 환경에서 활용 가능한 플러그인 개발 및 시연 단계(Step 8)로 이어질 예정이다.
단계
내용
진행 완료 여부
Step 1
프로젝트 목표 태스크 3가지 정의
완료
Step 2
평가 셋 제작
완료
Step 3
허밍 to MIDI 변환을 위한 모델 후보군 탐색 및 최종 결정
완료
Step 4
MIDI 후처리 보정을 위한 방법 탐색
완료
Step 5
모델 완성 및 피드백
미완료
Step 6
완성된 모델 평가셋에 적용 후 태스크 평가
미완료
Step 7
VST 플러그인 디자인 프로토타입 제작
완료
Step 8
플러그인 개발 및 시연
미완료

[표2] 프로젝트 진행 완료 여부

3-1. Step 1: 프로젝트 목표 태스크 3가지 정의
본 프로젝트의 핵심 목적은, 음성을 기반으로 한 작곡 지원 시스템에서 가장 중요한 요소인 보이스 투 미디 변환의 성능을 평가하고 개선하는 것이다. 기존 작곡 도구는 키보드나 악보 입력을 중심으로 구성되어 있어, 음악적 지식이 부족한 사용자에게 진입 장벽이 될 수 있다. 이에 따라 사용자가 허밍만으로도 직관적으로 멜로디를 입력하고, 이를 정확하게 미디로 변환할 수 있는 기술이 필요하다. 본 프로젝트는 이 변환 기술의 정확도 향상과 실용성 확보를 목표로 한다.
이러한 목표를 달성하기 위해 세 가지 태스크를 설정했으며, 이는 20개 이상의 평가 셋을 구축하고, 허밍을 미디로 변환했을 때의 정확도를 측정하며, 노이즈가 포함된 허밍을 미디로 변환했을 때의 정확도를 평가하는 것이다. 허밍 데이터는 사람마다 발성 방식, 리듬, 피치 감각이 다르기 때문에 표준화된 평가셋을 만드는 데 어려움이 있다. 이를 해결하기 위해 먼저 다양한 요소를 반영한 정량적 평가가 가능한 평가셋을 체계적으로 구축하였다. 이후 미디 변환 결과에 대해 정확한 비교 분석 기준을 마련하고, 이를 통해 최적의 모델과 변환 방식을 도출하였다.
첫 번째 태스크인 평가셋 구축 과정에서는 온음부터 32분음표, 점음표, 연음 등 다양한 음표를 포함하여 모델이 길이와 형태를 정확히 감지할 수 있도록 구성하였다. 또한 쉼표는 허밍 중간에 배치하여 음의 단절 여부를 평가하고, 강약 변화는 벨로시티 기반으로 구현하여 정교한 표현력을 테스트하였다. 여기에 노이즈가 포함된 허밍 데이터를 추가하여 노이즈 제거 성능을 평가할 수 있는 별도 셋도 구성하였다. 전체 데이터는 DAW(FL Studio)를 활용해 녹음·작성되었으며, 허밍은 무손실 WAV 형식, 정답 미디는 MIDI 형식으로 저장되었다.
두 번째 및 세 번째 태스크에서는 정량적 평가 기준에 따라 모델의 정확도를 측정하였다. 각 음에 대해 예측된 미디 노트가 정답 노트와 피치 및 지속 시간에서 얼마나 일치하는지를 기준으로 평가하며, 일치하지 않거나 과도하게 벗어난 경우 감점하는 방식으로 총 점수를 산출한다. 이 점수는 백분율로 환산되며, 이를 통해 최종 모델의 변환 성능을 정밀하게 평가한다.

3-2. Step 2: 평가 셋 제작
평가 셋 구축을 위해 고려한 주요 요소는 리듬 구분, 음 구분, 강약 구분으로, 이를 반영하여 총 21개의 평가 셋을 구성하였다. 먼저, 리듬 구분을 위해 음표와 쉼표를 개별적으로 평가하는 셋을 마련한다. 음표 구분을 위한 평가 셋은 온음표, 2분음표, 4분음표, 8분음표, 16분음표, 점2분음표, 점4분음표, 점8분음표, 3연음의 9가지로 구성된다. 쉼표 구분을 위한 평가 셋은 온쉼표, 2분쉼표, 4분쉼표, 8분쉼표, 16분쉼표의 5가지로 구성된다. 다음으로, 음 구분을 위해 반음 차이와 옥타브 차이를 각각 평가하는 2개의 평가 셋을 제작하였다. 또한, 강약 구분을 평가하기 위해 강약이 정박으로 반복되는 패턴을 포함한 1개의 평가 셋을 구성한다. 
마지막으로, 개별 요소를 복합적으로 평가하기 위해 실제 음악의 베이스 라인을 활용한다. Bon Jovi의 You Give Love A Bad Name은 8분음표 정박의 베이스 라인이 특징적인 곡으로, 이를 통해 음의 구분이 얼마나 정확하게 이루어지는지 평가한다. Queen의 Another One Bites The Dust는 쉼표를 활용해 리듬의 뉘앙스를 살리는 곡으로, 음표와 쉼표가 복합적으로 사용될 때 특징을 제대로 추출하는지를 평가하는 데 활용된다. Michael Jackson의 Billie Jean은 스타카토가 두드러지는 곡으로, 짧은 음의 길이가 정확하게 표현되는지를 평가한다. 또한, Tower of Power의 Mama Lied는 3연음을 기반으로 강약과 쉼표를 조합하여 음의 길이를 유지하는 곡으로, 이를 통해 복잡한 강약 변화를 얼마나 잘 반영하는지 평가한다. 표로 정리한 상세 계획은 [표3]과 같다.
요소
번호
평가 대상
구성 계획 내용
리듬의 구분
음표
1
온음표
4마디  4개의 온음표
2
2분음표
4마디 8개의 2분음표
3
4분음표
4마디 16개의 4분음표
4
8분음표
4마디 32개의 8분음표
5
16분음표
4마디 64개의 16분음표
6
점2분음표
점 2분음표와 4분음표의 조합 / 4마디 조합 4개
7
점4분음표
점 4분음표와 8분음표의 조합 / 4마디 조합 8개
8
점8분음표
점 8분음표와 16분음표의 조합 / 4마디 조합 16개
9
3연음
4마디 24개의 3연음
쉼표
1
온쉼표
온음표와 온쉼표의 조합 / 4마디 조합 2개
2
2분쉼표
2분음표와 2분쉼표의 조합 / 4마디 조합 4개
3
4분쉼표
4분음표와 4분쉼표의 조합 / 4마디 조합 8개
4
8분쉼표
8분음표와 8분쉼표의 조합 / 4마디 조합 16개
5
16분쉼표
16분음표와 16분쉼표의 조합 / 4마디 조합 32개
음의 구분
반음
1
-
반음 사이의 작은 차이의 음 구분
옥타브
2
-
같은 음 옥타브 차이 구분
강약 구분
1
-
4마디 8분음표에서 강약의 변화
종합
1
Bon Jovi - You Give Love A Bad Name
8분음표 정박 위주
2
Queen - Another Bites The Dust
쉼표의 사용
3
Michael Jackson - Billie Jean
스타카토의 짧은 음길이
4
Tower of Power - Mama Lied
3연음 위주의 강약의 변화


[표3] 평가셋 구성 상세 계획

3-3. Step 3: 허밍 to MIDI 변환을 위한 모델 후보군 탐색
3-3-1. 모델 후보군 탐색
보이스 투 미디 변환을 위한 최적의 모델을 선정하기 위해, 변환 정확도, 실시간 처리 가능성, 학습 데이터 요구량, 모델의 경량성을 기준으로 각 후보 모델을 비교 분석한다. 변환 정확도는 입력된 음성을 얼마나 정확하게 미디 데이터로 변환하는지를 평가하는 핵심 요소이며, 높은 정확도를 유지하는 모델이 이상적이다. 실시간 처리 가능성은 모델이 짧은 지연 시간 내에 변환을 수행할 수 있는지를 판단하는 기준으로, 사용자 경험에 직접적인 영향을 미친다. 또한, 모델이 높은 성능을 발휘하기 위해 얼마나 많은 학습 데이터를 요구하는지도 중요한 요소이며, 데이터 확보의 용이성과 학습 비용을 고려해야 한다. 모델의 경량성은 시스템 자원을 얼마나 효율적으로 활용하는지를 결정하는 요인이다. 이러한 기준을 바탕으로 여러 후보 모델의 장단점을 비교하고, 특정 환경과 요구사항에 가장 적합한 모델을 선정할 계획이다. 최종적으로, 실험을 통해 각 모델의 성능을 검증하고, 변환 정확도와 실시간 처리속도의 균형을 고려하여 각 변환 모델의 성능을 평가한다.
모델 후보군은 크게 세 가지 유형으로 나눌 수 있다. 전통적인 신호 처리 기법을 활용한 방식, 딥러닝 기반 접근법, 그리고 두 방법을 결합한 하이브리드 모델이다. 신호 처리 기법에는 FFT, STFT, YIN, HPS, MFCC, Envelope와 같은 방법들이 있으며, 딥러닝 기반 접근법에는 CNN, LSTM, Seq2Seq, GANs, Autoencoders, Transformer Models 등이 있다.
3-3-1-1. 전통적인 신호처리 기법
FFT는 Fast Fourier Transform의 약자로 신호를 주파수 영역으로 변환하는 전통적인 신호 처리 기법으로, 시간 도메인의 신호를 주파수 도메인으로 빠르게 변환할 수 있는 알고리즘이다. 이를 허밍에서 미디로 변환하는데 적용하면, 음성 신호의 주파수 성분을 분석하여 음조와 피치를 추출할 수 있다. 변환 정확도 측면에서는 FFT는 음성의 주파수 정보를 정확하게 추출할 수 있지만, 복잡한 음색이나 빠른 음의 변화에 대한 처리에는 한계가 있을 수 있다. 실시간 처리 가능성은 매우 뛰어나며, FFT 는 상대적으로 적은 계산 자원으로 수행되므로, 실시간 변환이 필요한 시스템에 적합하다. 학습 데이터 요구량은 기본적으로 적고, 신호 처리 기반 방법이므로 데이터 의존성이 낮다. 모델의 경량성 또한 다른 딥러닝 기법들에 비해 뛰어나 시스템 자원을 효율적으로 활용할 수 있다. 따라서, FFT는 실시간 성능이 중요한 시스템에서 유용하게 사용될 수 있으며, 변환 정확도를 보완할 수 있는 다른 기술들과 결합하여 활용될 가능성이 높다.
STFT는 Short-Time Fourier Transform의 약자로 신호를 시간과 주파수 두 차원에서 분석할 수 있는 기법으로, 신호를 일정 시간 구간으로 나누어 각 구간의 주파수 성분을 추출한다. 이를 허밍에서 미디로 변환하는 데 사용하면, 시간의 변화를 따라 주파수 성분을 분석함으로써 음성의 주파수 변화를 더 세밀하게 포착할 수 있다. 변환 정확도 측면에서 STFT는 시간 정보를 유지하면서 주파수 변화를 추적할 수 있어, 음성의 피치 변화를 잘 반영할 수 있다. 그러나 높은 시간 해상도와 주파수 해상도를 동시에 유지하는 데 한계가 있어, 매우 빠르게 변화하는 음성 신호의 정확한 분석에는 제약이 있을 수 있다. 실시간 처리 또한 빠른 편이다. 학습 데이터 요구량은 적으며, 추가적인 학습 데이터 없이도 기본적인 변환이 가능하다. 모델의 경량성 또한 뛰어나, 리소스가 제한된 환경에서도 잘 작동할 수 있다. STFT는 시간적으로 변하는 음성을 효과적으로 분석할 수 있어, 허밍과 같은 동적인 음성 데이터의 변환에 유리하며, 다른 신호 처리 기법이나 딥러닝 모델과 결합하여 성능을 개선할 수 있다.
YIN은 음성 신호에서 피치를 추출하는 데 사용되는 전통적인 알고리즘으로, 주로 음높이 탐지에 특화되어 있다. YIN 알고리즘은 주파수 영역에서의 진폭 변화를 기반으로 피치를 추정하며, 특히 노이즈가 적고 주파수 변화가 비교적 일정한 음성에서 뛰어난 성능을 보인다. 이를 허밍에서 미디로 변환할 때 적용하면, 허밍의 주된 피치를 정확하게 추출할 수 있어 음성의 기본적인 음높이를 효율적으로 감지할 수 있다. 변환 정확도 측면에서 YIN은 피치 추출에 매우 뛰어난 성능을 보이며, 특히 음의 정확한 피치를 잡아낼 수 있다. 그러나 복잡한 음색이나 여러 음이 동시에 발생하는 경우에는 제한적일 수 있다. 실시간 처리 가능성은 매우 높으며, 비교적 계산 복잡도가 낮아 빠르게 처리할 수 있다. 학습 데이터 요구량은 적고, 주로 알고리즘 자체에 의존하기 때문에 별도의 학습 데이터 없이도 효과적인 피치 추출이 가능하다. 또한 모델의 경량성 측면에서 매우 효율적이며, 제한된 자원 환경에서도 잘 작동할 수 있다. YIN은 음높이가 일정하거나 비교적 단순한 음성 신호에서 강력한 성능을 보이며, 미디 변환에 필수적인 피치 정보를 정확하게 추출하는 데 유리하다.
HPS(Harmonic Product Spectrum)는 음성 신호에서 피치를 추출하는 전통적인 신호 처리 기법으로, 주파수 영역에서 주파수 성분들이 주기적인 패턴을 보일 때 그 주기의 주파수를 감지하는 방식이다. HPS는 입력 신호의 주파수 스펙트럼을 여러 번 축소하여 곱한 후, 결과적으로 피크가 나타나는 지점을 찾아 피치를 추정하는 방법이다. 이 방식은 주파수 스펙트럼에서 고유의 주기적인 성분을 강조하는데 유리하여, 피치가 뚜렷한 신호에 대해 매우 높은 정확도를 제공한다. 변환 정확도 측면에서 HPS는 피치 추출에서 뛰어난 성능을 보이며, 특히 피치가 명확하게 구분되는 음성 신호에서 매우 효과적이다. 하지만 여러 음이 겹치는 복잡한 신호나, 빠르게 변화하는 피치의 경우 성능이 떨어질 수 있다. HPS는 비교적 간단한 수학적 연산으로 이루어져 있어 실시간으로 피치를 추출하는 데 적합하다. 또한 학습 데이터 요구량은 거의 없으며, 주로 신호의 주파수 성분을 분석하는 방식이기 때문에 별도의 대규모 데이터셋 없이도 효과적으로 적용할 수 있다. 모델의 경량성 또한 장점으로, 복잡한 연산이 필요하지 않아 제한된 자원 환경에서도 잘 동작한다. HPS는 음성이 일정하고 명확한 피치를 갖는 경우에 강력한 성능을 보인다. 허밍과 같은 단순한 음성 신호에서 높은 정확도로 피치를 추출할 수 있으며, 미디 변환을 위한 피치 추출 작업에 유리하다.
MFCC(Mel-Frequency Cepstral Coefficients)는 음성 신호의 특성을 효과적으로 추출하는 전통적인 신호 처리 기법으로, 주로 음성 인식에서 많이 사용된다. MFCC는 인간의 청각 특성을 반영하여, 주파수 대역을 로그 스케일로 변환하고 Mel 주파수 대역에 맞춰 필터를 적용한 후, 그 결과를 켑스트럼 계수로 변환하는 방식이다. 이 과정은 음성 신호의 주파수 성분을 효율적으로 추출하고, 특히 음색과 같은 중요한 음성 특징을 잘 반영할 수 있다. 변환 정확도 측면에서 MFCC는 음색 정보와 주파수 특성을 잘 추출할 수 있어 피치나 음의 고유한 특성을 파악하는 데 유리하다. 하지만 MFCC는 음색이나 스펙트럼의 변화를 잘 포착하지만, 단순한 피치 추출에는 상대적으로 제한적일 수 있다. MFCC는 계산 복잡도가 상대적으로 낮아 실시간 처리에 적합하다. 또한, 학습 데이터 요구량은 비교적 적다. MFCC는 기본적으로 알고리즘에 의해 계산되며, 별도의 대규모 데이터셋 없이도 효과적으로 음성 특성을 추출할 수 있다. 모델의 경량성 또한 장점으로, 계산 자원을 효율적으로 사용하여 제한된 환경에서도 동작할 수 있다. MFCC는 음성 신호의 특성을 잘 반영하며, 특히 음색과 음성의 세부적인 특성을 추출하는 데 강점이 있다. 그러나 피치 추출에 특화된 다른 방법들에 비해 피치 변환 정확도가 다소 떨어질 수 있으므로, 허밍과 같은 단순한 음성 신호의 피치를 추출하는 데에는 추가적인 기법과 결합하여 사용하는 것이 더 효과적일 수 있다.
Envelope(엔벨로프)는 음성 신호에서 진폭의 변화를 분석하는 신호 처리 기법으로, 신호의 시간적 변화를 파악하는 데 사용된다. 이 방법은 주로 신호의 진폭 변화를 추적하여 음향 신호의 크기나 세기, 동적인 변화를 표현하는 데 적합하다. 일반적으로 Envelope 추출은 신호의 파형을 분석하여 저주파 대역의 진폭 변화를 강조하고, 고주파 성분을 필터링하는 방식으로 이루어진다. 이러한 기법은 주로 신호의 발음이나 악기의 세기, 음의 변화 등을 파악하는 데 유용하다. 변환 정확도 측면에서 Envelope 기법은 신호의 진폭 변화를 잘 포착하지만, 피치나 음색의 세밀한 변화를 추적하는 데는 한계가 있을 수 있다. 따라서 허밍처럼 고정된 음높이와 음색을 가진 신호의 변환에는 적합하지만, 복잡하거나 빠르게 변화하는 신호에서는 정확도가 떨어질 수 있다. Envelope 추출은 비교적 간단한 수학적 연산으로 이루어져 있어 실시간으로 동작할 수 있으며, 실시간 피치 추출 작업에 적합하다. 학습 데이터 요구량은 적은 편이다. Envelope는 신호의 진폭 변화를 추적하는 방식이기 때문에, 별도의 학습 데이터 없이도 적용할 수 있는 장점이 있다. 모델의 경량성 또한 뛰어나, 복잡한 연산 없이 빠르게 처리할 수 있어 제한된 자원 환경에서도 잘 동작한다. Envelope 기법은 음성의 진폭이나 동적인 변화를 추적하는 데 유리하며, 특히 음성 신호의 세기나 패턴 변화를 분석하는 데 유용하다. 그러나 피치 추출과 관련된 정밀한 변환에는 다른 기법들과 결합하여 사용하는 것이 좋다.
3-3-1-2. 딥러닝 기반 신호처리 기법
Convolutional Neural Networks(CNN)은 이미지나 음성 신호와 같은 시퀀셜 데이터 처리에 매우 효과적인 딥러닝 모델로, 특히 패턴 인식에 강점을 가진다. 이 모델은 특히 시각적 특성이나 시간적 특성을 캡처하는 데 강력한 성능을 발휘하며, 허밍에서 미디로의 변환에서도 유용하게 사용될 수 있다. 변환 정확도 측면에서 CNN은 허밍 신호와 같은 주파수 특성을 잘 추출하고, 음성의 주기성이나 특정 음의 패턴을 정확하게 식별할 수 있다. 이러한 특성 덕분에 피치나 음색, 리듬 등 다양한 음악적 요소를 잘 인식하고 변환할 수 있다. 실시간 처리 가능성은 CNN이 여러 계층을 통한 복잡한 연산을 요구하지만, 하드웨어나 모델 최적화에 따라 실시간 처리도 가능하다. 다만, 고속 연산을 위해 GPU를 사용하는 것이 일반적이며, 실시간 처리를 구현하기 위해서는 추가적인 최적화가 필요할 수 있다. 학습 데이터 요구량은 상당히 큰 편이다. CNN은 성능을 최적화하기 위해 많은 양의 데이터가 필요하며, 다양한 허밍 데이터를 학습해야 높은 정확도를 달성할 수 있다. 또한, 모델이 자동으로 특성을 학습하기 때문에, 충분한 데이터와 학습 시간이 필요하다. 모델의 경량성은 상대적으로 낮은 편이다. 여러 층의 합성곱 연산과 파라미터가 많기 때문에 비교적 높은 계산 자원이 필요하다. 다만, 경량화된 CNN 모델을 사용하거나, 모델 압축 기술을 활용하면 경량화가 가능하다. CNN은 음성 신호의 중요한 패턴을 인식하고 변환하는 데 강력한 도구가 되며, 특히 복잡한 신호나 비정형적인 음성 데이터를 처리하는 데 유리하다. 다만, 큰 데이터셋과 연산 자원이 필요하며, 실시간 처리에서는 최적화가 필요할 수 있다.
Long Short-Term Memory (LSTM)은 순환 신경망(RNN)의 일종으로, 시간적 의존성을 가진 시퀀스 데이터를 처리하는 데 매우 효과적인 모델이다. 변환 정확도 측면에서 LSTM은 허밍에서 미디로 변환할 때, 피치와 리듬 등 시간적 특성을 잘 포착할 수 있다. LSTM의 특성상, 이전 프레임의 정보를 기억하면서 현재 프레임을 처리하기 때문에, 노이즈가 포함된 데이터에서도 시간적으로 일관성 있는 결과를 생성할 수 있다. 실시간 처리 가능성은 LSTM이 시퀀스 데이터의 순차적인 처리를 필요로 하기 때문에, 실시간 처리에 약간의 지연이 발생할 수 있다. 그러나, 적절한 모델 최적화와 하드웨어 지원을 통해 실시간 처리가 가능하다. 학습 데이터 요구량은 상대적으로 높은 편이다. LSTM은 시퀀스 데이터의 시간적 패턴을 학습하기 위해 긴 데이터와 충분한 학습 시간이 필요하다. 특히, 허밍과 같은 데이터는 다양한 시간적 변화를 포함하고 있기 때문에, 이를 잘 처리하기 위한 대규모 데이터셋이 필요하다. 모델의 경량성은 LSTM이 상대적으로 많은 파라미터를 필요로 하기 때문에, 계산 자원을 많이 소모할 수 있다. 다만, 모델 압축 기법을 사용하거나, 적절한 하이퍼파라미터 튜닝을 통해 경량화가 가능하다. LSTM은 시간적 특성을 고려한 시퀀스 데이터 처리에 매우 강력하며, 허밍을 미디로 변환하는 데 적합하다. 다만, 대규모 데이터셋과 계산 자원이 필요하고, 실시간 처리를 위한 최적화가 필요하다.
Sequence-to-Sequence (Seq2Seq) 모델은 인코더는 입력 시퀀스를 고정 길이의 벡터로 인코딩하고, 디코더는 이 벡터를 기반으로 출력 시퀀스를 생성한다. 허밍을 미디로 변환하는 작업에서는, Seq2Seq 모델이 각 음표의 시간적 변화와 음높이를 시퀀스 형태로 학습하여, 입력된 허밍 음성의 특성을 미디 데이터로 정확하게 변환하는 데 도움을 준다. 변환 정확도 측면에서 Seq2Seq는 각 프레임의 음성 정보를 시퀀스로 처리하여, 허밍의 피치와 리듬을 변환된 미디에 잘 반영할 수 있다. 특히, 디코더는 정확한 음표를 예측하기 위해 입력 시퀀스의 정보를 세밀하게 처리한다. 실시간 처리 가능성은 Seq2Seq 모델이 상대적으로 긴 시퀀스를 처리해야 하므로, 실시간 처리에는 일정한 지연이 발생할 수 있다. 하지만, 모델 최적화나 하드웨어 가속을 통해 이를 개선할 수 있다. 학습 데이터 요구량은 많은 양의 데이터셋이 필요하다. Seq2Seq 모델은 시퀀스를 변환하기 위해 충분한 학습 데이터가 있어야 제대로 된 예측 성능을 보인다. 특히, 허밍과 미디 간의 변환을 학습하기 위해 다양한 음성과 리듬을 포함한 데이터가 필요하다. 모델의 경량성은 Seq2Seq 모델이 많은 파라미터를 포함할 수 있어 상대적으로 계산 자원이 많이 소모된다. 그러나, 하이퍼파라미터 튜닝과 모델 압축 기법을 통해 경량화가 가능하다. Seq2Seq 모델은 시퀀스를 변환하는 데 강력한 성능을 발휘하며, 허밍을 미디로 변환하는 데 유용하다. 다만, 많은 양의 학습 데이터와 계산 자원이 필요하며, 실시간 처리 속도를 개선하기 위한 최적화가 필요하다.
Generative Adversarial Networks (GANs)는 생성 모델 중 하나로, 두 개의 신경망인 생성자(Generator)와 판별자(Discriminator)가 서로 경쟁하는 방식으로 학습하는 모델이다. 생성자는 가짜 데이터를 생성하려고 시도하고, 판별자는 그 데이터가 진짜인지 가짜인지를 구별하려고 한다. GANs는 이미지 생성, 스타일 변환, 텍스트 생성 등 다양한 분야에서 뛰어난 성능을 보여주고 있다. 허밍을 미디로 변환하는 작업에서 GANs는 생성자가 허밍 음성을 미디 데이터로 변환하려고 시도하고, 판별자가 이 미디 데이터가 실제 허밍의 특징을 잘 반영하는지를 평가하는 방식으로 작동할 수 있다. GANs의 큰 장점은 매우 사실적인 데이터 생성을 가능하게 한다는 점이다. 변환 정확도 측면에서 GANs는 실제 허밍과 변환된 미디 간의 차이를 줄여, 자연스러운 결과물을 생성할 수 있다. 실시간 처리 가능성은 GANs가 상대적으로 복잡한 구조를 가지고 있어 실시간 처리에 어려움을 겪을 수 있다. 그러나, 모델 최적화와 하드웨어 가속을 통해 이러한 문제를 완화할 수 있다. 학습 데이터 요구량은 GANs가 고품질의 데이터를 요구하는 모델이기 때문에, 충분한 학습 데이터셋이 필요하다. 특히, 다양한 허밍과 미디 간의 매핑을 학습하기 위해서는 다양한 음성 데이터를 확보하는 것이 중요하다. GANs는 또한 계산 자원이 많이 소모되므로, 경량화가 어려운 단점이 있지만, 최근에는 경량화된 GAN 모델들이 제안되고 있어 이를 활용할 수 있다. GANs는 매우 고유한 방식으로 데이터 생성을 가능하게 하며, 허밍을 미디로 변환하는 데 유망한 모델로 활용될 수 있다. 다만, 실시간 처리나 학습 데이터의 요구량, 모델의 경량성 측면에서 최적화가 필요하며, 이를 통해 높은 변환 정확도와 효율적인 처리 속도를 달성할 수 있다.
Transformer 모델은 자연어 처리(NLP)에서 처음 도입된 모델로, 주로 입력 데이터의 시퀀스를 처리하는 데 강력한 성능을 발휘한다. Transformer는 특히 시퀀스의 순서를 중요시하는 작업에 뛰어나므로, 허밍의 리듬과 음의 순차적 변화를 잘 파악할 수 있다. 이를 바탕으로 허밍에서 미디로의 변환을 정교하게 수행할 수 있으며, 특히 미디 변환 과정에서 발생하는 패턴을 학습하는 데 강점을 가진다. 변환 정확도 측면에서 Transformer는 매우 우수한 성능을 보인다. 특히, 복잡한 시퀀스 변환에서 뛰어난 성능을 발휘하며, 다양한 패턴을 잘 학습할 수 있다. 그러나 Transformer 모델은 학습에 많은 데이터를 필요로 하고, 훈련 시간이 길어질 수 있다는 단점이 있다. 또한, 대규모 모델을 사용할 경우 높은 연산 자원과 메모리를 요구하므로, 경량화가 중요한 시스템에서는 자원 소모가 클 수 있다. 실시간 처리 가능성은 Transformer가 긴 시퀀스를 처리하는 데 시간이 많이 소요될 수 있다는 점에서 다소 부족할 수 있으나, 최근의 경량화된 Transformer 변형 모델들은 실시간 처리에 적합하도록 설계되고 있다. Transformer 모델은 높은 변환 정확도와 뛰어난 패턴 인식 능력을 제공하며, 특히 복잡한 허밍 데이터를 미디로 변환하는 데 강력한 모델이 될 수 있다. 그러나 학습 데이터 요구량이 많고, 자원 소모가 크기 때문에 실시간 처리와 경량화가 중요한 경우에는 다른 모델들과의 결합이나 최적화가 필요할 수 있다.

3-3-1-3. 모델 후보군 탐색 요약
위 내용을 미리 설정한 평가 기준에 따라 정리한 내용은 다음 [표4]와 같다.
모델
변환 정확도
실시간 처리 가능성
학습 데이터 요구량
모델의 경량성
FFT
중간
높음
낮음
높음
STFT
중간
중간
낮음
중간
YIN
높음
높음
낮음
높음
HPS
높음
중간
낮음
높음
MFCC
중간
중간
중간
중간
Envelope
중간
높음
낮음
높음
CNN
높음
중간
높음
낮음
LSTM
높음
낮음
높음
낮음
Seq2Seq
높음
낮음
높음
낮음
GANs
높음
낮음
매우 높음
낮음
Autoencoders
높음
중간
높음
낮음
Transformer Models
매우 높음
낮음
매우 높음
낮음

[표4] 각 모델 예상 요인 평가
3-3-2. 모델 후보군 테스트 결과
실제로 모델 후보군들을 테스트 해본 결과, 허밍을 미디로 변환하는 데에 정확도를 높이기 위한 복잡한 모델을 탐색할 필요가 없음을 깨달았다. 실험 결과, [그림1]과 [그림2]와 같이 간단한 알고리즘인 FFT나 YIN 기반 방식만으로도 충분히 높은 정확도를 달성할 수 있었으며, 이는 본 시스템이 이산적인 기준으로 정확도를 평가하기 때문에 가능한 일이다. 실제로 일정 범위 내의 음정 오차는 같은 음으로 인식되어 정답으로 처리되므로, 상대적으로 가벼운 모델도 성능 저하 없이 안정적인 결과를 보여준다. 반면, [그림3]과 같이 HPS와 같은 알고리즘은 잡음, 고조파, 무음 구간 등에서 민감하게 반응하여 부정확한 결과를 도출하였고, 이러한 특성 때문에 이번 과제에서는 적합하지 않은 것으로 판단되었다. 결국, 본 실험은 오히려 적합하지 않은 방식들을 걸러내는 데 의미가 있었으며, 불필요하게 복잡한 모델을 도입하기보다는 상황에 맞는 경량 모델을 선택하는 것이 더 효과적이라는 결론을 내릴 수 있었다.

[그림1] FFT 기반 허밍 to MIDI 변환

[그림2] yin 기반 허밍 to MIDI 변환

[그림3] hps 기반 허밍 to MIDI 변환

3-4. Step 4: MIDI 후처리 보정을 위한 방법 탐색
여러 필터를 순차적으로 적용해 MIDI 데이터를 보정하는 방식을 테스트 해보기 위해 너무 짧은 노트를 제거하고, 박자를 정렬하고, 3) 튀는 피치를 제거한 후에, 같은 피치를 연결하는 스무딩 과정을 거쳤다.. 우선 지나치게 짧은 노트는 실제 연주나 노래에서 의미 없는 노이즈일 가능성이 높기 때문에 필터링을 통해 제거한다. 이후 각 노트의 시작 시점과 길이를 16분음표와 같은 단위로 정렬하여 리듬을 정돈하고, 박자에 맞지 않게 어긋난 음들을 바로잡는다. 그런 다음, 앞뒤 음들과 비교했을 때 유난히 튀는 음정이나 급격한 변화가 있는 노트를 찾아 제거함으로써 멜로디 흐름의 부자연스러움을 줄인다. 마지막으로 같은 피치의 노트들이 거의 연속적으로 등장하는 경우에는 이를 하나로 병합하여 보다 부드럽고 자연스러운 연속음을 만들고자 한다. 이러한 과정을 통해 MIDI 데이터는 보다 안정적이고 음악적으로 의미 있는 형태로 정제된다

[그림4] 필터링 전후 MIDI 노트 비교 예시
이러한 필터링 방식을 테스트 해본 결과 딥러닝 기반 MIDI 후처리 방식이 필요하다는 것을 깨달았다. 기존의 음성 기반 MIDI 변환 시스템은 매우 짧은 시간 내에 결과물을 도출할 수 있고, 음성 데이터를 MIDI로 높은 정확도로 변환할 수 있다는 장점을 가진다. 그러나 [그림4]와 같이 기계적으로 정확한 처리 방식은 사용자 녹음 중의 실수나 의도치 않은 음까지 그대로 반영하는 한계를 갖는다. 그 결과, 실수로 발생한 불필요한 음이 정제되지 않은 채 포함되어, 사용자 의도와 어긋난 결과가 나오는 경우도 적지 않다. 이러한 문제를 해결하기 위해, 딥러닝 기반의 MIDI 후처리 기법을 도입한다. 이를 위해 먼저 MIDI 데이터를 모델이 이해할 수 있는 형식으로 전환해야 하며, 그 과정에서 시계열 데이터로 변환한 뒤 각 이벤트를 토큰화하여 모델 입력에 적합한 형태로 구성한다. 입력 데이터는 보정되지 않은 원본 MIDI 또는 필터링을 거친 MIDI를 사용할 수 있으며, 출력 데이터는 사람이 직접 수정한 고품질 MIDI를 정답으로 삼는다.
하지만 현재 이러한 형태의 입력-출력 쌍으로 구성된 학습 데이터는 존재하지 않기 때문에, 프로젝트를 통해 학습용 데이터를 직접 생성하기 위하여 기존의 완성도 높은 MIDI 데이터를 기반으로 다양한 방식으로 변조를 가해, 의도적으로 오류나 노이즈를 포함시킨 데이터를 생성한다. 이렇게 만들어진 변조 MIDI를 입력으로, 원본 MIDI를 정답으로 설정하여 학습 데이터를 구성함으로써, 모델이 보정 전 MIDI를 입력받아 사람처럼 자연스럽게 수정·정제된 형태로 후처리할 수 있도록 학습시킨다.

3-5. Step 7: VST 플러그인 디자인 프로토타입 제작

[그림5] 사용자 인터페이스 프로토타입 화면
[그림5]의 프로그램 디자인 프로토타입 예시는 사용자의 허밍을 기반으로 자동 생성된 MIDI 데이터를 시각적으로 확인하고 편집할 수 있도록 구성되어 있다. 상단에는 녹음된 오디오의 파형이 표시되어 있어 전체 음성 입력 흐름을 쉽게 파악할 수 있으며, 하단에는 각 음에 해당하는 MIDI 노트가 피아노 롤 형태로 시각화되어 있어 직관적인 편집이 가능하다. 좌측에는 BPM과 박자 설정, 피치 스무딩 강도 및 노이즈 제거 정도, 리듬 정렬(퀀타이징) 등의 후처리 파라미터를 실시간으로 조절할 수 있는 슬라이더가 배치되어 있어, 사용자가 직접 보정 강도를 조절해가며 원하는 결과를 얻을 수 있도록 설계되었다. 이러한 인터페이스는 음악적인 전문 지식이 없는 사용자도 간단히 조작할 수 있도록 직관성과 조작성에 중점을 두었으며, 허밍 기반 작곡의 접근성을 높이는 데 기여할 수 있을 것으로 기대된다.


4. 진행 예정 내용
현재 프로젝트는 Step 4까지의 기술적 준비 작업과 Step 7의 인터페이스 설계까지 완료된 상태이며, 이후 단계에서는 이를 바탕으로 모델 성능 평가와 실제 작곡 도구로의 구현에 초점을 맞출 예정이다. 우선, Step 5에서는 기존에 탐색 및 결정된 모델을 기반으로 허밍 입력을 미디로 변환하는 최종 모델을 완성하고, 이를 반복적인 피드백 과정을 통해 보정 및 최적화한다. 이후 Step 6에서는 완성된 모델을 구축된 평가셋에 적용하여 세 가지 태스크(기본 허밍 변환 정확도, 노이즈 포함 변환 정확도, 리듬 및 피치 단위 정밀도)에 대한 성능을 정량적으로 평가할 예정이다.
이와 병행하여, Step 8에서는 이미 설계가 완료된 VST 플러그인 UI를 기반으로 본격적인 기능 구현에 들어간다. 해당 플러그인은 허밍 입력을 실시간으로 처리하여 미디로 변환된 데이터를 DAW(디지털 오디오 워크스테이션) 환경에서 바로 확인하고 작곡에 활용할 수 있도록 지원한다. 최종적으로, 시스템 시연을 통해 직관적인 작곡 도구로서의 실효성을 검증하고, 실제 사용자에게 제공 가능한 형태로의 완성도를 끌어올리는 것이 목표이다.

Reference
[1] "고속 푸리에 변환." Wikipedia, Wikimedia Foundation, https://ko.wikipedia.org/wiki/고속_푸리에_변환. Accessed 18 Apr. 2025.
[2] "Short-time Fourier transform." Wikipedia, Wikimedia Foundation, https://en.wikipedia.org/wiki/Short-time_Fourier_transform. Accessed 18 Apr. 2025.
[3] de Cheveigné, Alain, and Hideki Kawahara. "YIN, a Fundamental Frequency Estimator for Speech and Music." The Journal of the Acoustical Society of America, vol. 111, no. 4, Apr. 2002, pp. 1917–1930.
[4] Smyth, Tamara. "Harmonic Product Spectrum." Music 270a: Signal Analysis, Department of Music, University of California, San Diego, 2 Dec. 2019, https://musicweb.ucsd.edu/~trsmyth/analysis/Harmonic_Product_Spectrum.html. Accessed 18 Apr. 2025.
[5] "Mel-frequency cepstrum." Wikipedia, Wikimedia Foundation, https://en.wikipedia.org/wiki/Mel-frequency_cepstrum. Accessed 18 Apr. 2025.
[6] Kotnik, Bojan, et al. "Evaluation of Pitch Detection Algorithms in Adverse Conditions." Academia.edu, https://www.academia.edu/73642009/Evaluation_of_Pitch_Detection_Algorithms_in_Adverse_Conditions. Accessed 18 Apr. 2025.
[7] "Convolutional Neural Network란? 꼭 알아야 할 3가지 사항." MathWorks, https://kr.mathworks.com/discovery/convolutional-neural-network.html. Accessed 18 Apr. 2025.
[8] "장단기 메모리." Wikipedia, Wikimedia Foundation, https://ko.wikipedia.org/wiki/장단기_메모리. Accessed 18 Apr. 2025.
[9] 김성훈. "14-01 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)." 딥 러닝을 이용한 자연어 처리 입문, 위키독스, https://wikidocs.net/24996. Accessed 18 Apr. 2025.
[10] "Generative adversarial network." Wikipedia, Wikimedia Foundation, https://en.wikipedia.org/wiki/Generative_adversarial_network. Accessed 18 Apr. 2025.
[11] "TuneNN: A transformer-based network model for pitch detection." Hacker News, 19 Dec. 2023, https://news.ycombinator.com/item?id=38694719. Accessed 18 Apr. 2025.
